{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhDhLw0jivoHc+t0DiRALh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dniboghgnis/Tensorboard-101/blob/master/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-IWPtJuUgQG",
        "outputId": "90d430cd-e232-4395-da82-662de99a3b3c"
      },
      "source": [
        "import codecs\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import requests\n",
        "from gensim.models import Word2Vec\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.activations import softmax\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
        "\n",
        "########################################################################################################################\n",
        "########################################### DATA PREPARATION ###########################################################\n",
        "########################################################################################################################\n",
        "url = 'http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip'\n",
        "r = requests.get(url)\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()\n",
        "\n",
        "\n",
        "def get_all_conversations():\n",
        "    all_conversations = []\n",
        "    with codecs.open(\"./cornell movie-dialogs corpus/movie_lines.txt\",\n",
        "                     \"rb\",\n",
        "                     encoding=\"utf-8\",\n",
        "                     errors=\"ignore\") as f:\n",
        "        lines = f.read().split(\"\\n\")\n",
        "        for line in lines:\n",
        "            all_conversations.append(line.split(\" +++$+++ \"))\n",
        "    return all_conversations\n",
        "\n",
        "\n",
        "def get_all_sorted_chats(all_conversations):\n",
        "    all_chats = {}\n",
        "    # get only first 10000 conversations from dataset because whole dataset will take 9.16 TiB of RAM\n",
        "    for tokens in all_conversations[:5000]:\n",
        "        if len(tokens) > 4:\n",
        "            all_chats[int(tokens[0][1:])] = tokens[4]\n",
        "    return sorted(all_chats.items(), key=lambda x: x[0])\n",
        "\n",
        "\n",
        "def clean_text(text_to_clean):\n",
        "    res = text_to_clean.lower()\n",
        "    res = re.sub(r\"i'm\", \"i am\", res)\n",
        "    res = re.sub(r\"he's\", \"he is\", res)\n",
        "    res = re.sub(r\"she's\", \"she is\", res)\n",
        "    res = re.sub(r\"it's\", \"it is\", res)\n",
        "    res = re.sub(r\"that's\", \"that is\", res)\n",
        "    res = re.sub(r\"what's\", \"what is\", res)\n",
        "    res = re.sub(r\"where's\", \"where is\", res)\n",
        "    res = re.sub(r\"how's\", \"how is\", res)\n",
        "    res = re.sub(r\"\\'ll\", \" will\", res)\n",
        "    res = re.sub(r\"\\'ve\", \" have\", res)\n",
        "    res = re.sub(r\"\\'re\", \" are\", res)\n",
        "    res = re.sub(r\"\\'d\", \" would\", res)\n",
        "    res = re.sub(r\"\\'re\", \" are\", res)\n",
        "    res = re.sub(r\"won't\", \"will not\", res)\n",
        "    res = re.sub(r\"can't\", \"cannot\", res)\n",
        "    res = re.sub(r\"n't\", \" not\", res)\n",
        "    res = re.sub(r\"n'\", \"ng\", res)\n",
        "    res = re.sub(r\"'bout\", \"about\", res)\n",
        "    res = re.sub(r\"'til\", \"until\", res)\n",
        "    res = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", res)\n",
        "    return res\n",
        "\n",
        "\n",
        "def get_conversation_dict(sorted_chats):\n",
        "    conv_dict = {}\n",
        "    counter = 1\n",
        "    conv_ids = []\n",
        "    for i in range(1, len(sorted_chats) + 1):\n",
        "        if i < len(sorted_chats):\n",
        "            if (sorted_chats[i][0] - sorted_chats[i - 1][0]) == 1:\n",
        "                if sorted_chats[i - 1][1] not in conv_ids:\n",
        "                    conv_ids.append(sorted_chats[i - 1][1])\n",
        "                conv_ids.append(sorted_chats[i][1])\n",
        "            elif (sorted_chats[i][0] - sorted_chats[i - 1][0]) > 1:\n",
        "                conv_dict[counter] = conv_ids\n",
        "                conv_ids = []\n",
        "            counter += 1\n",
        "        else:\n",
        "            continue\n",
        "    return conv_dict\n",
        "\n",
        "\n",
        "def get_clean_q_and_a(conversations_dictionary):\n",
        "    ctx_and_target = []\n",
        "    for current_conv in conversations_dictionary.values():\n",
        "        if len(current_conv) % 2 != 0:\n",
        "            current_conv = current_conv[:-1]\n",
        "        for i in range(0, len(current_conv), 2):\n",
        "            ctx_and_target.append((current_conv[i], current_conv[i + 1]))\n",
        "    context, target = zip(*ctx_and_target)\n",
        "    context_dirty = list(context)\n",
        "    clean_questions = list()\n",
        "    for i in range(len(context_dirty)):\n",
        "        clean_questions.append(clean_text(context_dirty[i]))\n",
        "    target_dirty = list(target)\n",
        "    clean_answers = list()\n",
        "    for i in range(len(target_dirty)):\n",
        "        clean_answers.append('<START> '\n",
        "                             + clean_text(target_dirty[i])\n",
        "                             + ' <END>')\n",
        "    return clean_questions, clean_answers\n",
        "\n",
        "\n",
        "conversations = get_all_conversations()\n",
        "total = len(conversations)\n",
        "print(\"Total conversations in dataset: {}\".format(total))\n",
        "all_sorted_chats = get_all_sorted_chats(conversations)\n",
        "conversation_dictionary = get_conversation_dict(all_sorted_chats)\n",
        "questions, answers = get_clean_q_and_a(conversation_dictionary)\n",
        "print(\"Questions in dataset: {}\".format(len(questions)))\n",
        "print(\"Answers in dataset: {}\".format(len(answers)))\n",
        "\n",
        "########################################################################################################################\n",
        "############################################# MODEL TRAINING ###########################################################\n",
        "########################################################################################################################\n",
        "\n",
        "target_regex = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n\\'0123456789'\n",
        "tokenizer = Tokenizer(filters=target_regex)\n",
        "tokenizer.fit_on_texts(questions + answers)\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary size : {}'.format(VOCAB_SIZE))\n",
        "\n",
        "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
        "maxlen_questions = max([len(x) for x in tokenized_questions])\n",
        "encoder_input_data = pad_sequences(tokenized_questions,\n",
        "                                   maxlen=maxlen_questions,\n",
        "                                   padding='post')\n",
        "\n",
        "print(encoder_input_data.shape)\n",
        "\n",
        "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
        "maxlen_answers = max([len(x) for x in tokenized_answers])\n",
        "decoder_input_data = pad_sequences(tokenized_answers,\n",
        "                                   maxlen=maxlen_answers,\n",
        "                                   padding='post')\n",
        "print(decoder_input_data.shape)\n",
        "\n",
        "for i in range(len(tokenized_answers)):\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "padded_answers = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "decoder_output_data = to_categorical(padded_answers, VOCAB_SIZE)\n",
        "\n",
        "print(decoder_output_data.shape)\n",
        "\n",
        "enc_inputs = Input(shape=(None,))\n",
        "enc_embedding = Embedding(VOCAB_SIZE, 200, mask_zero=True)(enc_inputs)\n",
        "_, state_h, state_c = LSTM(200, return_state=True)(enc_embedding)\n",
        "enc_states = [state_h, state_c]\n",
        "\n",
        "dec_inputs = Input(shape=(None,))\n",
        "dec_embedding = Embedding(VOCAB_SIZE, 200, mask_zero=True)(dec_inputs)\n",
        "dec_lstm = LSTM(200, return_state=True, return_sequences=True)\n",
        "dec_outputs, _, _ = dec_lstm(dec_embedding, initial_state=enc_states)\n",
        "dec_dense = Dense(VOCAB_SIZE, activation=softmax)\n",
        "output = dec_dense(dec_outputs)\n",
        "\n",
        "model = Model([enc_inputs, dec_inputs], output)\n",
        "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit([encoder_input_data, decoder_input_data],\n",
        "          decoder_output_data,\n",
        "          batch_size=50,\n",
        "          epochs=300)\n",
        "model.save('model_test_1.h5')\n",
        "\n",
        "\n",
        "# model.load_weights('model_big.h5')\n",
        "\n",
        "\n",
        "def make_inference_models():\n",
        "    dec_state_input_h = Input(shape=(200,))\n",
        "    dec_state_input_c = Input(shape=(200,))\n",
        "    dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n",
        "    dec_outputs, state_h, state_c = dec_lstm(dec_embedding,\n",
        "                                             initial_state=dec_states_inputs)\n",
        "    dec_states = [state_h, state_c]\n",
        "    dec_outputs = dec_dense(dec_outputs)\n",
        "    dec_model = Model(\n",
        "        inputs=[dec_inputs] + dec_states_inputs,\n",
        "        outputs=[dec_outputs] + dec_states)\n",
        "    print('Inference decoder:')\n",
        "    dec_model.summary()\n",
        "    print('Inference encoder:')\n",
        "    enc_model = Model(inputs=enc_inputs, outputs=enc_states)\n",
        "    enc_model.summary()\n",
        "    return enc_model, dec_model\n",
        "\n",
        "\n",
        "def str_to_tokens(sentence: str):\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for current_word in words:\n",
        "        result = tokenizer.word_index.get(current_word, '')\n",
        "        if result != '':\n",
        "            tokens_list.append(result)\n",
        "    return pad_sequences([tokens_list],\n",
        "                         maxlen=maxlen_questions,\n",
        "                         padding='post')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total conversations in dataset: 304714\n",
            "Questions in dataset: 2347\n",
            "Answers in dataset: 2347\n",
            "Vocabulary size : 4998\n",
            "(2347, 223)\n",
            "(2347, 132)\n",
            "(2347, 132, 4998)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 200)    999600      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 200)    999600      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 200), (None, 320800      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 200),  320800      embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 4998)   1004598     lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 3,645,398\n",
            "Trainable params: 3,645,398\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/300\n",
            "47/47 [==============================] - 117s 2s/step - loss: 0.6302\n",
            "Epoch 2/300\n",
            " 5/47 [==>...........................] - ETA: 1:39 - loss: 0.5562"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvNIKNhHUvhG"
      },
      "source": [
        "enc_model, dec_model = make_inference_models()\n",
        "\n",
        "for _ in range(100):\n",
        "    states_values = enc_model.predict(\n",
        "        str_to_tokens(input('Enter question : ')))\n",
        "    empty_target_seq = np.zeros((1, 1))\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition:\n",
        "        dec_outputs, h, c = dec_model.predict([empty_target_seq]\n",
        "                                              + states_values)\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if sampled_word_index == index:\n",
        "                if word != 'end':\n",
        "                    decoded_translation += ' {}'.format(word)\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'end' \\\n",
        "                or len(decoded_translation.split()) \\\n",
        "                > maxlen_answers:\n",
        "            stop_condition = True\n",
        "\n",
        "        empty_target_seq = np.zeros((1, 1))\n",
        "        empty_target_seq[0, 0] = sampled_word_index\n",
        "        states_values = [h, c]\n",
        "\n",
        "    print(decoded_translation)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}